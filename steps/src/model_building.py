from typing import List, Tuple

import lxml
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import statsmodels.api as sm
from numpy import sqrt
from scipy.stats import shapiro
from sklearn.dummy import DummyRegressor
from sklearn.metrics import make_scorer, mean_squared_error
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from statsmodels.formula.api import ols
from statsmodels.graphics.gofplots import qqplot
from statsmodels.stats.outliers_influence import variance_inflation_factor


class DataSplitter:
    """A class used to split data into training and testing parts"""

    def __init__(self, df: pd.DataFrame, features: List[str], target: str, test_size: float = 0.2):
        """
        Args:
        df: pandas DataFrame, the entire dataset.
        features: list of str, the column names to be used as features.
        target: str, the column name to be used as target.
        test_size: float, proportion of the dataset to include in the test split.
        """
        self.df = df
        self.features = features
        self.target = target
        self.test_size = test_size

    def split(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """Splits data into training and testing parts.
        
        Returns:
        Tuple of pandas DataFrame and Series: (X_train, X_test, y_train, y_test)
        """ 
        # drop month_year and id columns 
        X = self.df[self.features]
        y = self.df[self.target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, shuffle=False)
        return X_train, X_test, y_train, y_test


class LinearRegressionModel:
    """A class used to train a linear regression model"""

    def __init__(self, X_train: pd.DataFrame, y_train: pd.Series):
        """
        Args:
        X_train: pandas DataFrame, the training features.
        y_train: pandas Series, the training target.
        """
        self.X_train = X_train
        self.y_train = y_train

    def train(self):
        """Trains a linear regression model using statsmodels.

        Returns:
        statsmodels.regression.linear_model.RegressionResultsWrapper: the trained model
        """
        X_train = sm.add_constant(self.X_train)  # Adding a constant (intercept term) to the model 
        print(X_train)

        model = sm.OLS(self.y_train, X_train)
        results = model.fit() 
        print(results.summary())
        return results

class ModelRefinement:
    
    def __init__(self, model, data):
        self.model = model
        self.data = data
        self.predictors = [x for x in self.model.model.exog_names if x != 'const']
        self.target = self.model.model.endog_names


    def remove_insignificant_vars(self, alpha=0.05):
        summary = self.model.summary().tables[1]
        summary_df = pd.DataFrame(summary.data)
        summary_df.columns = summary_df.iloc[0]
        summary_df = summary_df.drop(0)
        summary_df = summary_df.set_index(summary_df.columns[0])
        summary_df['P>|t|'] = summary_df['P>|t|'].astype(float)
        significant_vars = [var for var in self.predictors if summary_df.loc[var, 'P>|t|'] < alpha]
        self.predictors = significant_vars
        return significant_vars  # Added this line



    def check_multicollinearity(self):
        exog = sm.add_constant(self.data[self.predictors])
        vif = pd.Series([variance_inflation_factor(exog.values, i) 
                         for i in range(exog.shape[1])], 
                        index=exog.columns)
        print("Variance Inflation Factors:")
        print(vif)
        
    def check_normality_of_residuals(self):
        residuals = self.model.resid
        qqplot(residuals, line='s')
        plt.show()
        stat, p = shapiro(residuals)
        print('Statistics=%.3f, p=%.3f' % (stat, p))
        alpha = 0.05
        if p > alpha:
            print('Sample looks Gaussian (fail to reject H0)')
        else:
            print('Sample does not look Gaussian (reject H0)')

    def check_homoscedasticity(self):
        residuals = self.model.resid
        plt.scatter(self.model.predict(), residuals)
        plt.xlabel('Predicted')
        plt.ylabel('Residual')
        plt.axhline(y=0, color='red')
        plt.title('Residual vs. Predicted')
        plt.show()

    def validate(self, k=10):
        kf = KFold(n_splits=k)
        y = self.data["qty"]
        X = sm.add_constant(self.data[self.predictors])
        errors = []
        
        for train, test in kf.split(X):
            model = sm.OLS(y.iloc[train], X.iloc[train]).fit()
            predictions = model.predict(X.iloc[test])
            mse = mean_squared_error(y.iloc[test], predictions)
            errors.append(mse) 
            # print error at each fold 
            print(f"MSE: {mse}")
        
        rmse = np.sqrt(np.mean(errors))
        self.rmse = rmse
        return rmse
    



class BaselineModel:
    def __init__(self, X, y):
        self.X = X
        self.y = y
        self.model = DummyRegressor(strategy='mean')

    def train(self):
        self.model.fit(self.X, self.y)

    def validate(self, k=10):
        mse_scorer = make_scorer(mean_squared_error)
        mse_scores = cross_val_score(self.model, self.X, self.y, cv=k, scoring=mse_scorer)
        rmse_scores = sqrt(mse_scores)
        print(f"Baseline MSE: {mse_scores.mean()}")
        print(f"Baseline RMSE: {rmse_scores.mean()}")



class InteractionEffects:

    def __init__(self, data: pd.DataFrame):
        """
        Args:
        data: pandas DataFrame, the data which might contain interacting variables.
        """
        self.data = data.copy()

    def add_interaction(self, var1: str, var2: str):
        """Adds an interaction term to the data.

        Args:
        var1: str, name of the first interacting variable
        var2: str, name of the second interacting variable

        Returns: 
        pandas DataFrame, the data with the added interaction term.
        """
        interaction_term = self.data[var1] * self.data[var2]
        self.data[f'{var1}:{var2}'] = interaction_term

    def get_data(self):
        """Returns the data with interaction terms.

        Returns: 
        pandas DataFrame, the data with the added interaction terms.
        """
        return self.data

if __name__ == "__main__": 
    # df = pd.read_csv("/Users/ayushsingh/Desktop/MLProjectPackages/retail-price-optimization/data/retail_prices_encoded_date.csv") 
    # df.drop(["month_year", "id"], axis=1, inplace=True)
    # X = df.drop(["qty"], axis=1) 
    # y = df["qty"]
    # data_splitter = DataSplitter(df, X.columns, y.name) 
    # X_train, X_test, y_train, y_test = data_splitter.split()
    # model = LinearRegressionModel(X_train, y_train)
    # results = model.train()
    # print(results.summary())
    # refinement1 = ModelRefinement(results, df)
    # predictors = refinement1.remove_insignificant_vars(alpha=0.05)  # removes insignificant variables 
    # print(predictors) 
    # X_train_significant = X_train[predictors] 
    # lr_model_2 = LinearRegressionModel(X_train_significant, y_train) 
    # df_with_sig_vars = pd.concat([X_train_significant, y_train], axis=1) 
    # df_with_sig_vars.to_csv("/Users/ayushsingh/Desktop/MLProjectPackages/retail-price-optimization/data/retail_prices_encoded_date_sig_vars.csv", index=False) 
    # model = lr_model_2.train() 
    # print(model.summary())

    # refinement = ModelRefinement(model, df)

    # # Now you can use the methods defined in the class
    # # predictors = refinement.remove_insignificant_vars(alpha=0.05)  # removes insignificant variables
    # refinement.check_multicollinearity()  # checks multicollinearity among predictors
    # refinement.check_normality_of_residuals()  # checks if residuals are normally distributed
    # refinement.check_homoscedasticity()  # checks if residuals have constant variance
    # refinement.validate(k=10)  # cross-validates the model using k-fold cross-validation

    # # Assume target_var is the name of your target variable
    # target_var = 'qty'

    # # Calculate the target variable's standard deviation, mean and median
    # std_dev = np.std(refinement.data[target_var])
    # mean_value = np.mean(refinement.data[target_var])
    # median_value = np.median(refinement.data[target_var])

    # # Print the comparison
    # print(f"Standard Deviation of {target_var}: {std_dev}")
    # print(f"Mean of {target_var}: {mean_value}")
    # print(f"Median of {target_var}: {median_value}")
    # print(f"RMSE of the model: {refinement.rmse}")

    # baseline = BaselineModel(X, y)
    # baseline.train()
    # baseline.validate(k=10)

    # # Initialize the class
    # interaction_effects = InteractionEffects(df)

    # # Add interaction terms
    # interaction_effects.add_interaction('var1', 'var2')  # replace 'var1' and 'var2' with the names of your interacting variables

    # # Get the data with interaction terms
    # df_with_interaction = interaction_effects.get_data()
    pass 